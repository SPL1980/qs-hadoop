1) Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        很明显应该是HADOOP_HOME的问题。如果HADOOP_HOME为空，必然fullExeName为null\bin\winutils.exe。
        解决方法很简单，配置环境变量，不想重启电脑可以在程序里加上：
        System.setProperty("hadoop.home.dir", "E:\\Program Files\\hadoop-2.7.0");
        注：E:\\Program Files\\hadoop-2.7.0是我本机解压的hadoop的路径。
        稍后再执行，你可能还是会出现同样的错误，这个时候你可能会要怪我了。
        其实一开始我是拒绝的，因为你进入你的hadoop-x.x.x/bin目录下看，你会发现你压根就没有winutils.exe这个东东。
        于是我告诉你，你可以去github下载一个，地球人都知道的地址发你一个。
        地址：https://github.com/srccodes/hadoop-common-2.2.0-bin
        不要顾虑它的版本，不用怕，因为我用的最新的hadoop-2.7.0都没问题！下载好后，把winutils.exe加入你的hadoop-x.x.x/bin下。

2）qs-hadoop-sparkSQL 坑点：
    1） com.qs.SQLContextApp 测试这个功能的时候，
        val dataFrame = sqlContext.read.format("json").load(path)
        从参数传进来的path 不能再系统的根目录下比如 E:/game.json (错误 ==>> Can not create a Path from an empty string)
        需要修改一下目录 如：E:/json/game.json

3)scala 隐式转换
        val sparkSession = SparkSession.builder()
          .appName("DataFrameRDDApp")
          .master("local[2]")
          .getOrCreate()

        var path : String = "file:///E:\\json\\files.txt"
        if (args.length > 0) path = args(0)

        //RDD ==> DataFrame
        val rdd = sparkSession.sparkContext.textFile(path)

        import sparkSession.implicits._  //隐式转换 toDF()

        val peopleDF = rdd.map(_.split(","))
          .map(line => Info(line(0).toInt, line(1), line(2).toInt, line(3).toDouble)).toDF()

4) 用java API 编程做kafka producer 的时候，Windows操作系统 hosts 文件没有添加
    指向kafka服务端的映射，  启动生产者，连接不上kafka。
    添加映射之后就可以了。（未发现为何出现此问题）

    --- 网上人家的回答：
    2、/kafka_2.11-0.9.0.0/config/server.properties
    listerners 需配置ip ,不能配置主机名，因本地Hosts中不存在对应的Ip配置，导致producer 无法连接
    官网默认提示 设置host.name advertised.host.name 为Ip 时可以通过ip连接，但经过测试，此配置失败(经测试成功)。
    （经过测试，我可以成功，不用此步骤  --->> ）只有通过修改Listerners 后成功。

5) kafka producer 的
        //定义消息的key和value的数据类型都是字节数组
        properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");//IntegerDeserializer
        properties.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
   要与 consumer 的
        properties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        properties.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
   匹配，否者订阅不到消息。

6)  启动hadoop或者关闭的时候（./start-all.sh 等命令时候）
    Error: Cannot find configuration directory:xxx
    Error: Cannot find configuration directory: xxx
    export HADOOP_CONF_DIR=/opt/hadoop-2.7.1/etc/hadoop/ （正确）
    （必须为全目录，不能为变量拼接）
    export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop/ （错误）

7）启动hbase的时候出现这个错误（原因：环境变量配置错了，少了一个“/”）
    Error: Could not find or load main class org.apache.hadoop.hbase.util.HBaseConfTool
    Error: Could not find or load main class org.apache.hadoop.hbase.zookeeper.ZKServerTool

8) 启动spark-shell 的时候报异常：
    Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState'
    仔细查找输出日志发现原因是：
        Name node is in safe mode
    解决办法：bin/hadoop dfsadmin -safemode leave

    参考文章如下：
    错误Name node is in safe mode的解决方法
    将本地文件拷贝到hdfs上去，结果上错误：Name node is in safe mode
    这是因为在分布式文件系统启动的时候，开始的时候会有安全模式，当分布式文件系统处于安全模式的情况下，文件系统中的内容不允许修改也不允许删除，直到安全模式结束。安全模式主要是为了系统启动的时候检查各个DataNode上数据块的有效性，同时根据策略必要的复制或者删除部分数据块。运行期通过命令也可以进入安全模式。在实践过程中，系统启动的时候去修改和删除文件也会有安全模式不允许修改的出错提示，只需要等待一会儿即可。
    可以通过以下命令来手动离开安全模式：

    bin/hadoop dfsadmin -safemode leave
    用户可以通过dfsadmin -safemode value 来操作安全模式，参数value的说明如下：
    enter - 进入安全模式
    leave - 强制NameNode离开安全模式
    get - 返回安全模式是否开启的信息
    wait - 等待，一直到安全模式结束。

9) idea 编译报错：scalac error: bad option: '-make:transitive' on mvn package via command line
    参考：
    1 问题描述：
    ubuntu环境下用eclipse+maven开发scala的时候出现错误：scalac error: bad option: '-make:transitive' on mvn package via command line
    2 解决方法：
    （1）打开pom.xml，删除
           <parameter value="-make:transitive"/>
    （2）添加dependance

        <dependency>
            <groupId>org.specs2</groupId>
            <artifactId>specs2_2.11</artifactId>
            <version>2.4.6</version>
            <scope>test</scope>
        </dependency>
