1) Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        很明显应该是HADOOP_HOME的问题。如果HADOOP_HOME为空，必然fullExeName为null\bin\winutils.exe。
        解决方法很简单，配置环境变量，不想重启电脑可以在程序里加上：
        System.setProperty("hadoop.home.dir", "E:\\Program Files\\hadoop-2.7.0");
        注：E:\\Program Files\\hadoop-2.7.0是我本机解压的hadoop的路径。
        稍后再执行，你可能还是会出现同样的错误，这个时候你可能会要怪我了。
        其实一开始我是拒绝的，因为你进入你的hadoop-x.x.x/bin目录下看，你会发现你压根就没有winutils.exe这个东东。
        于是我告诉你，你可以去github下载一个，地球人都知道的地址发你一个。
        地址：https://github.com/srccodes/hadoop-common-2.2.0-bin
        不要顾虑它的版本，不用怕，因为我用的最新的hadoop-2.7.0都没问题！下载好后，把winutils.exe加入你的hadoop-x.x.x/bin下。

2）qs-hadoop-sparkSQL 坑点：
    1） com.qs.SQLContextApp 测试这个功能的时候，
        val dataFrame = sqlContext.read.format("json").load(path)
        从参数传进来的path 不能再系统的根目录下比如 E:/game.json (错误 ==>> Can not create a Path from an empty string)
        需要修改一下目录 如：E:/json/game.json

3)scala 隐式转换
        val sparkSession = SparkSession.builder()
          .appName("DataFrameRDDApp")
          .master("local[2]")
          .getOrCreate()

        var path : String = "file:///E:\\json\\files.txt"
        if (args.length > 0) path = args(0)

        //RDD ==> DataFrame
        val rdd = sparkSession.sparkContext.textFile(path)

        import sparkSession.implicits._  //隐式转换 toDF()

        val peopleDF = rdd.map(_.split(","))
          .map(line => Info(line(0).toInt, line(1), line(2).toInt, line(3).toDouble)).toDF()

4) 用java API 编程做kafka producer 的时候，Windows操作系统 hosts 文件没有添加
    指向kafka服务端的映射，  启动生产者，连接不上kafka。
    添加映射之后就可以了。（未发现为何出现此问题）

    --- 网上人家的回答：
    2、/kafka_2.11-0.9.0.0/config/server.properties
    listerners 需配置ip ,不能配置主机名，因本地Hosts中不存在对应的Ip配置，导致producer 无法连接
    官网默认提示 设置host.name advertised.host.name 为Ip 时可以通过ip连接，但经过测试，此配置失败(经测试成功)。
    （经过测试，我可以成功，不用此步骤  --->> ）只有通过修改Listerners 后成功。

5) kafka producer 的
        //定义消息的key和value的数据类型都是字节数组
        properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");//IntegerDeserializer
        properties.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
   要与 consumer 的
        properties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        properties.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
   匹配，否者订阅不到消息。

6)  启动hadoop或者关闭的时候（./start-all.sh 等命令时候）
    Error: Cannot find configuration directory:xxx
    Error: Cannot find configuration directory: xxx
    export HADOOP_CONF_DIR=/opt/hadoop-2.7.1/etc/hadoop/ （正确）
    （必须为全目录，不能为变量拼接）
    export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop/ （错误）

7）启动hbase的时候出现这个错误（原因：环境变量配置错了，少了一个“/”）
    Error: Could not find or load main class org.apache.hadoop.hbase.util.HBaseConfTool
    Error: Could not find or load main class org.apache.hadoop.hbase.zookeeper.ZKServerTool
